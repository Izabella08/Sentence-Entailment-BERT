{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0.ipynb","provenance":[],"mount_file_id":"1QkPDe0M9bFBRePekVg-ztj3sedtL01ZY","authorship_tag":"ABX9TyPW8Z15EGAlk8qx16G+cSTo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"150d016c58a340b9b593323ee7e273bb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6fd88041cce24b2e9fbb085b526a3fd1","IPY_MODEL_9428bde832844a7abcaa43f6de54b169","IPY_MODEL_27f8b804535848e0a5da21b04faff6f2"],"layout":"IPY_MODEL_cb386d961d7e4b8c8f2d478934831fde"}},"6fd88041cce24b2e9fbb085b526a3fd1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0d2af253024c4dc6965d6c1315ce48a9","placeholder":"​","style":"IPY_MODEL_8a962fa571624ea6a3c31b9a564884d0","value":"Downloading: 100%"}},"9428bde832844a7abcaa43f6de54b169":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fe56f878b5c541dc8be89bf2e3e1040e","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_357128c094ab4febad33985699f868a2","value":570}},"27f8b804535848e0a5da21b04faff6f2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aa138100aa4648e5b5d9df3243b52242","placeholder":"​","style":"IPY_MODEL_e040ca72d8864cdda58aa95ca277d0f5","value":" 570/570 [00:00&lt;00:00, 17.2kB/s]"}},"cb386d961d7e4b8c8f2d478934831fde":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0d2af253024c4dc6965d6c1315ce48a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a962fa571624ea6a3c31b9a564884d0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fe56f878b5c541dc8be89bf2e3e1040e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"357128c094ab4febad33985699f868a2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"aa138100aa4648e5b5d9df3243b52242":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e040ca72d8864cdda58aa95ca277d0f5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7e43cd119b7f4963958441f53c177f33":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1620d1aa9117418487218121af026a57","IPY_MODEL_bd5c021d19fe4655bc3ff2e4b674ab1c","IPY_MODEL_5da6f94ce4e648079d2a1a6c53e6b021"],"layout":"IPY_MODEL_a563a1301dd14917a4994cf9a12ba72b"}},"1620d1aa9117418487218121af026a57":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5bc025b044b748e8aa8d6949c108a040","placeholder":"​","style":"IPY_MODEL_0d41ab2360cf4c5f991c9609637422f3","value":"Downloading: 100%"}},"bd5c021d19fe4655bc3ff2e4b674ab1c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d1a00fc94bad446fb53fd00ac7357357","max":536063208,"min":0,"orientation":"horizontal","style":"IPY_MODEL_68567efabd6f4d04b1e1bc3051eb7eb9","value":536063208}},"5da6f94ce4e648079d2a1a6c53e6b021":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_57b4dfa4f4e84a8d89794c7ebdb83dd8","placeholder":"​","style":"IPY_MODEL_7b8fb61e8b5b407aa30d8b467bd71e25","value":" 511M/511M [00:09&lt;00:00, 57.0MB/s]"}},"a563a1301dd14917a4994cf9a12ba72b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5bc025b044b748e8aa8d6949c108a040":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0d41ab2360cf4c5f991c9609637422f3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d1a00fc94bad446fb53fd00ac7357357":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"68567efabd6f4d04b1e1bc3051eb7eb9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"57b4dfa4f4e84a8d89794c7ebdb83dd8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b8fb61e8b5b407aa30d8b467bd71e25":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e84da1577d0e44e280df7618042ec4e2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_668f83cbcb764c8e83aa2b3cee2e72c3","IPY_MODEL_f99c4c0d15984e28bc07fa809a4eb700","IPY_MODEL_2eac15d8b276479682ee13cabe240ef7"],"layout":"IPY_MODEL_d5d4f623e78c44c8a3916ad0f320e9ce"}},"668f83cbcb764c8e83aa2b3cee2e72c3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_357ce185231e4f7aabe35fc1434d2326","placeholder":"​","style":"IPY_MODEL_56158667aa6347f1b3271bee14e390f7","value":"Downloading: 100%"}},"f99c4c0d15984e28bc07fa809a4eb700":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4199ed6f66db4a5fb24bb68a05ffc070","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ef5557986f8e499593adef2d7401d0c3","value":231508}},"2eac15d8b276479682ee13cabe240ef7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2c2fa187791c401cb4e6cb7c4884bb17","placeholder":"​","style":"IPY_MODEL_b9c4942333984e37ac36c9a6f3fc98b4","value":" 226k/226k [00:00&lt;00:00, 5.53MB/s]"}},"d5d4f623e78c44c8a3916ad0f320e9ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"357ce185231e4f7aabe35fc1434d2326":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"56158667aa6347f1b3271bee14e390f7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4199ed6f66db4a5fb24bb68a05ffc070":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef5557986f8e499593adef2d7401d0c3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2c2fa187791c401cb4e6cb7c4884bb17":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9c4942333984e37ac36c9a6f3fc98b4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fc7ae6ba276042a082d0e47365c18a40":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3dcd6aee66d64afc91314dc127494b21","IPY_MODEL_9d76db7e8dd44f7aad2f9c8ffc5a7ada","IPY_MODEL_0e7f40248fe0482db7a6a141ce7af197"],"layout":"IPY_MODEL_42b1069f5f2f4e38b64f5e3d6ca24b59"}},"3dcd6aee66d64afc91314dc127494b21":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a0534687aaa44a7abd5384a70296c7b7","placeholder":"​","style":"IPY_MODEL_f1631f03f546423ab89701ab4ded68da","value":"Downloading: 100%"}},"9d76db7e8dd44f7aad2f9c8ffc5a7ada":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_806daa5631cb4e06b8879e9f967e6347","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4e6073ffdad04430a42aa7478cb52245","value":28}},"0e7f40248fe0482db7a6a141ce7af197":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_119c023db5604e39bc090aa383224abe","placeholder":"​","style":"IPY_MODEL_101180770b254d31b737857853f906d9","value":" 28.0/28.0 [00:00&lt;00:00, 871B/s]"}},"42b1069f5f2f4e38b64f5e3d6ca24b59":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a0534687aaa44a7abd5384a70296c7b7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f1631f03f546423ab89701ab4ded68da":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"806daa5631cb4e06b8879e9f967e6347":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e6073ffdad04430a42aa7478cb52245":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"119c023db5604e39bc090aa383224abe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"101180770b254d31b737857853f906d9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["!pip install transformers\n","!pip install sentencepiece"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-LbYhzMfS5pC","executionInfo":{"status":"ok","timestamp":1652981103612,"user_tz":-180,"elapsed":17485,"user":{"displayName":"Izabella Bartalus","userId":"14250698818363907854"}},"outputId":"366715df-f174-4e4d-f00f-cfd1a838fd15"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n","\u001b[K     |████████████████████████████████| 4.2 MB 31.6 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 67.1 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 59.6 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.6.0-py3-none-any.whl (84 kB)\n","\u001b[K     |████████████████████████████████| 84 kB 4.4 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.6.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.2\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 15.2 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.96\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"brKR1YsGSpHB","executionInfo":{"status":"ok","timestamp":1652981114203,"user_tz":-180,"elapsed":6226,"user":{"displayName":"Izabella Bartalus","userId":"14250698818363907854"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import transformers"]},{"cell_type":"code","source":["max_length = 128  # Maximum length of input sentence to the model.\n","batch_size = 32\n","epochs = 2\n","\n","# Labels in our dataset.\n","labels = [\"NOT ENTAILMENT - Unknown\", \"Entailment\", \"NOT ENTAILMENT - Contradiction\"]"],"metadata":{"id":"mPMp-xAHTAAR","executionInfo":{"status":"ok","timestamp":1652981116252,"user_tz":-180,"elapsed":502,"user":{"displayName":"Izabella Bartalus","userId":"14250698818363907854"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["PATH = '/content/drive/MyDrive/Colab Notebooks/ambiguous_knights_knaves.json'"],"metadata":{"id":"J9lX78wvTIUh","executionInfo":{"status":"ok","timestamp":1652981117995,"user_tz":-180,"elapsed":12,"user":{"displayName":"Izabella Bartalus","userId":"14250698818363907854"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["import json\n","def read_jsonl_file(PATH):\n","  df = pd.read_json(PATH)\n","\n","  with open(PATH,'r') as f:\n","    data = json.loads(f.read())\n","\n","  df_nested_list = pd.json_normalize(data=data['puzzles'], record_path='QA', meta=['puzzle_text'])\n","  return df_nested_list"],"metadata":{"id":"ukkIuuOxTObP","executionInfo":{"status":"ok","timestamp":1652981119838,"user_tz":-180,"elapsed":14,"user":{"displayName":"Izabella Bartalus","userId":"14250698818363907854"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["df = pd.DataFrame(read_jsonl_file(PATH))"],"metadata":{"id":"BMsfLMcWTd4B","executionInfo":{"status":"ok","timestamp":1652981122103,"user_tz":-180,"elapsed":473,"user":{"displayName":"Izabella Bartalus","userId":"14250698818363907854"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# SE IMPARTE DATASET-UL IN TRAIN, VALIDARE SI TEST\n","train_df = df[['qid','puzzle_text', 'question', 'answer']][:700]\n","valid_df  = df[['qid','puzzle_text', 'question', 'answer']]\n","test_df  = df[['qid','puzzle_text', 'question', 'answer']]\n","\n","# Shape of the data\n","print(f\"Total train samples : {train_df.shape[0]}\")\n","print(f\"Total validation samples: {valid_df.shape[0]}\")\n","print(f\"Total test samples: {test_df.shape[0]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OhJqTUn7TVB9","executionInfo":{"status":"ok","timestamp":1652981124060,"user_tz":-180,"elapsed":9,"user":{"displayName":"Izabella Bartalus","userId":"14250698818363907854"}},"outputId":"1d8c63c9-6b77-4af1-d08e-043cfd75042d"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Total train samples : 700\n","Total validation samples: 940\n","Total test samples: 940\n"]}]},{"cell_type":"code","source":["print(\"Train Target Distribution\")\n","print(train_df.answer.value_counts())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v92cgmPXTjMB","executionInfo":{"status":"ok","timestamp":1652981125909,"user_tz":-180,"elapsed":12,"user":{"displayName":"Izabella Bartalus","userId":"14250698818363907854"}},"outputId":"352efc67-49a0-4958-dc15-35e732e0621e"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Train Target Distribution\n","NOT ENTAILMENT - Unknown          536\n","Entailment                         82\n","NOT ENTAILMENT - Contradiction     82\n","Name: answer, dtype: int64\n"]}]},{"cell_type":"code","source":["print(\"Validation Target Distribution\")\n","print(valid_df.answer.value_counts())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AOoRdf1qTpAs","executionInfo":{"status":"ok","timestamp":1652981127948,"user_tz":-180,"elapsed":391,"user":{"displayName":"Izabella Bartalus","userId":"14250698818363907854"}},"outputId":"2c7b862b-7efb-4f3b-aa8a-7c9dc90cd9ad"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Validation Target Distribution\n","NOT ENTAILMENT - Unknown          730\n","Entailment                        105\n","NOT ENTAILMENT - Contradiction    105\n","Name: answer, dtype: int64\n"]}]},{"cell_type":"code","source":["train_df = (\n","    train_df[train_df.answer != \"-\"]\n","    .sample(frac=1.0, random_state=42)\n","    .reset_index(drop=True)\n",")\n","valid_df = (\n","    valid_df[valid_df.answer != \"-\"]\n","    .sample(frac=1.0, random_state=42)\n","    .reset_index(drop=True)\n",")"],"metadata":{"id":"ZbwfjGH2Ttvb","executionInfo":{"status":"ok","timestamp":1652981129873,"user_tz":-180,"elapsed":13,"user":{"displayName":"Izabella Bartalus","userId":"14250698818363907854"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["train_df[\"label\"] = train_df[\"answer\"].apply(\n","    lambda x: 0 if x == \"NOT ENTAILMENT - Contradiction\" else 1 if x == \"Entailment\" else 2\n",")\n","y_train = tf.keras.utils.to_categorical(train_df.label, num_classes=3)\n","\n","valid_df[\"label\"] = valid_df[\"answer\"].apply(\n","    lambda x: 0 if x == \"NOT ENTAILMENT - Contradiction\" else 1 if x == \"Entailment\" else 2\n",")\n","y_val = tf.keras.utils.to_categorical(valid_df.label, num_classes=3)\n","\n","test_df[\"label\"] = test_df[\"answer\"].apply(\n","    lambda x: 0 if x == \"NOT ENTAILMENT - Contradiction\" else 1 if x == \"Entailment\" else 2\n",")\n","y_test = tf.keras.utils.to_categorical(test_df.label, num_classes=3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KBxEdcI0TxL4","executionInfo":{"status":"ok","timestamp":1652981131170,"user_tz":-180,"elapsed":20,"user":{"displayName":"Izabella Bartalus","userId":"14250698818363907854"}},"outputId":"c1d4f836-452a-45cf-dfdd-1a317466afe8"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  if sys.path[0] == '':\n"]}]},{"cell_type":"code","source":["class BertSemanticDataGenerator(tf.keras.utils.Sequence):\n","    \"\"\"Generates batches of data.\n","\n","    Args:\n","        sentence_pairs: Array of premise and hypothesis input sentences.\n","        labels: Array of labels.\n","        batch_size: Integer batch size.\n","        shuffle: boolean, whether to shuffle the data.\n","        include_targets: boolean, whether to incude the labels.\n","\n","    Returns:\n","        Tuples `([input_ids, attention_mask, `token_type_ids], labels)`\n","        (or just `[input_ids, attention_mask, `token_type_ids]`\n","         if `include_targets=False`)\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        sentence_pairs,\n","        labels,\n","        batch_size=batch_size,\n","        shuffle=True,\n","        include_targets=True,\n","    ):\n","        self.sentence_pairs = sentence_pairs\n","        self.labels = labels\n","        self.shuffle = shuffle\n","        self.batch_size = batch_size\n","        self.include_targets = include_targets\n","        # Load our BERT Tokenizer to encode the text.\n","        # We will use base-base-uncased pretrained model.\n","        self.tokenizer = transformers.BertTokenizer.from_pretrained(\n","            \"bert-base-uncased\", do_lower_case=True\n","        )\n","        self.indexes = np.arange(len(self.sentence_pairs))\n","        self.on_epoch_end()\n","\n","    def __len__(self):\n","        # Denotes the number of batches per epoch.\n","        return len(self.sentence_pairs) // self.batch_size\n","\n","    def __getitem__(self, idx):\n","        # Retrieves the batch of index.\n","        indexes = self.indexes[idx * self.batch_size : (idx + 1) * self.batch_size]\n","        sentence_pairs = self.sentence_pairs[indexes]\n","\n","        # With BERT tokenizer's batch_encode_plus batch of both the sentences are\n","        # encoded together and separated by [SEP] token.\n","        encoded = self.tokenizer.batch_encode_plus(\n","            sentence_pairs.tolist(),\n","            add_special_tokens=True,\n","            max_length=max_length,\n","            return_attention_mask=True,\n","            return_token_type_ids=True,\n","            pad_to_max_length=True,\n","            return_tensors=\"tf\",\n","        )\n","\n","        # Convert batch of encoded features to numpy array.\n","        input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n","        attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n","        token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n","\n","        # Set to true if data generator is used for training/validation.\n","        if self.include_targets:\n","            labels = np.array(self.labels[indexes], dtype=\"int32\")\n","            return [input_ids, attention_masks, token_type_ids], labels\n","        else:\n","            return [input_ids, attention_masks, token_type_ids]\n","\n","    def on_epoch_end(self):\n","        # Shuffle indexes after each epoch if shuffle is set to True.\n","        if self.shuffle:\n","            np.random.RandomState(42).shuffle(self.indexes)"],"metadata":{"id":"V0ACX8zOT9Re","executionInfo":{"status":"ok","timestamp":1652981133558,"user_tz":-180,"elapsed":14,"user":{"displayName":"Izabella Bartalus","userId":"14250698818363907854"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# Create the model under a distribution strategy scope.\n","strategy = tf.distribute.MirroredStrategy()\n","\n","with strategy.scope():\n","    # Encoded token ids from BERT tokenizer.\n","    input_ids = tf.keras.layers.Input(\n","        shape=(max_length,), dtype=tf.int32, name=\"input_ids\"\n","    )\n","    # Attention masks indicates to the model which tokens should be attended to.\n","    attention_masks = tf.keras.layers.Input(\n","        shape=(max_length,), dtype=tf.int32, name=\"attention_masks\"\n","    )\n","    # Token type ids are binary masks identifying different sequences in the model.\n","    token_type_ids = tf.keras.layers.Input(\n","        shape=(max_length,), dtype=tf.int32, name=\"token_type_ids\"\n","    )\n","    # Loading pretrained BERT model.\n","    bert_model = transformers.TFBertModel.from_pretrained(\"bert-base-uncased\")\n","    # Freeze the BERT model to reuse the pretrained features without modifying them.\n","    bert_model.trainable = False\n","\n","    bert_output = bert_model(\n","        input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids\n","    )\n","    sequence_output = bert_output.last_hidden_state\n","    pooled_output = bert_output.pooler_output\n","    # Add trainable layers on top of frozen layers to adapt the pretrained features on the new data.\n","    bi_lstm = tf.keras.layers.Bidirectional(\n","        tf.keras.layers.LSTM(64, return_sequences=True)\n","    )(sequence_output)\n","    # Applying hybrid pooling approach to bi_lstm sequence output.\n","    avg_pool = tf.keras.layers.GlobalAveragePooling1D()(bi_lstm)\n","    max_pool = tf.keras.layers.GlobalMaxPooling1D()(bi_lstm)\n","    concat = tf.keras.layers.concatenate([avg_pool, max_pool])\n","    dropout = tf.keras.layers.Dropout(0.3)(concat)\n","    output = tf.keras.layers.Dense(3, activation=\"softmax\")(dropout)\n","    model = tf.keras.models.Model(\n","        inputs=[input_ids, attention_masks, token_type_ids], outputs=output\n","    )\n","\n","    model.compile(\n","        optimizer=tf.keras.optimizers.Adam(),\n","        loss=\"categorical_crossentropy\",\n","        metrics=[\"acc\"],\n","    )\n","\n","\n","print(f\"Strategy: {strategy}\")\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["150d016c58a340b9b593323ee7e273bb","6fd88041cce24b2e9fbb085b526a3fd1","9428bde832844a7abcaa43f6de54b169","27f8b804535848e0a5da21b04faff6f2","cb386d961d7e4b8c8f2d478934831fde","0d2af253024c4dc6965d6c1315ce48a9","8a962fa571624ea6a3c31b9a564884d0","fe56f878b5c541dc8be89bf2e3e1040e","357128c094ab4febad33985699f868a2","aa138100aa4648e5b5d9df3243b52242","e040ca72d8864cdda58aa95ca277d0f5","7e43cd119b7f4963958441f53c177f33","1620d1aa9117418487218121af026a57","bd5c021d19fe4655bc3ff2e4b674ab1c","5da6f94ce4e648079d2a1a6c53e6b021","a563a1301dd14917a4994cf9a12ba72b","5bc025b044b748e8aa8d6949c108a040","0d41ab2360cf4c5f991c9609637422f3","d1a00fc94bad446fb53fd00ac7357357","68567efabd6f4d04b1e1bc3051eb7eb9","57b4dfa4f4e84a8d89794c7ebdb83dd8","7b8fb61e8b5b407aa30d8b467bd71e25"]},"id":"26pIhg5kUA6R","executionInfo":{"status":"ok","timestamp":1652981160592,"user_tz":-180,"elapsed":23351,"user":{"displayName":"Izabella Bartalus","userId":"14250698818363907854"}},"outputId":"b6204faf-75c8-4ee1-d06d-db405078f551"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"150d016c58a340b9b593323ee7e273bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/511M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e43cd119b7f4963958441f53c177f33"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n","- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","Strategy: <tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7fe3f7169590>\n","Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_ids (InputLayer)         [(None, 128)]        0           []                               \n","                                                                                                  \n"," attention_masks (InputLayer)   [(None, 128)]        0           []                               \n","                                                                                                  \n"," token_type_ids (InputLayer)    [(None, 128)]        0           []                               \n","                                                                                                  \n"," tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_ids[0][0]',              \n","                                thPoolingAndCrossAt               'attention_masks[0][0]',        \n","                                tentions(last_hidde               'token_type_ids[0][0]']         \n","                                n_state=(None, 128,                                               \n","                                 768),                                                            \n","                                 pooler_output=(Non                                               \n","                                e, 768),                                                          \n","                                 past_key_values=No                                               \n","                                ne, hidden_states=N                                               \n","                                one, attentions=Non                                               \n","                                e, cross_attentions                                               \n","                                =None)                                                            \n","                                                                                                  \n"," bidirectional (Bidirectional)  (None, 128, 128)     426496      ['tf_bert_model[0][0]']          \n","                                                                                                  \n"," global_average_pooling1d (Glob  (None, 128)         0           ['bidirectional[0][0]']          \n"," alAveragePooling1D)                                                                              \n","                                                                                                  \n"," global_max_pooling1d (GlobalMa  (None, 128)         0           ['bidirectional[0][0]']          \n"," xPooling1D)                                                                                      \n","                                                                                                  \n"," concatenate (Concatenate)      (None, 256)          0           ['global_average_pooling1d[0][0]'\n","                                                                 , 'global_max_pooling1d[0][0]']  \n","                                                                                                  \n"," dropout_37 (Dropout)           (None, 256)          0           ['concatenate[0][0]']            \n","                                                                                                  \n"," dense (Dense)                  (None, 3)            771         ['dropout_37[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 109,909,507\n","Trainable params: 427,267\n","Non-trainable params: 109,482,240\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","source":["train_data = BertSemanticDataGenerator(\n","    train_df[[\"puzzle_text\", \"question\"]].values.astype(\"str\"),\n","    y_train,\n","    batch_size=batch_size,\n","    shuffle=True,\n",")\n","valid_data = BertSemanticDataGenerator(\n","    valid_df[[\"puzzle_text\", \"question\"]].values.astype(\"str\"),\n","    y_val,\n","    batch_size=batch_size,\n","    shuffle=False,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["e84da1577d0e44e280df7618042ec4e2","668f83cbcb764c8e83aa2b3cee2e72c3","f99c4c0d15984e28bc07fa809a4eb700","2eac15d8b276479682ee13cabe240ef7","d5d4f623e78c44c8a3916ad0f320e9ce","357ce185231e4f7aabe35fc1434d2326","56158667aa6347f1b3271bee14e390f7","4199ed6f66db4a5fb24bb68a05ffc070","ef5557986f8e499593adef2d7401d0c3","2c2fa187791c401cb4e6cb7c4884bb17","b9c4942333984e37ac36c9a6f3fc98b4","fc7ae6ba276042a082d0e47365c18a40","3dcd6aee66d64afc91314dc127494b21","9d76db7e8dd44f7aad2f9c8ffc5a7ada","0e7f40248fe0482db7a6a141ce7af197","42b1069f5f2f4e38b64f5e3d6ca24b59","a0534687aaa44a7abd5384a70296c7b7","f1631f03f546423ab89701ab4ded68da","806daa5631cb4e06b8879e9f967e6347","4e6073ffdad04430a42aa7478cb52245","119c023db5604e39bc090aa383224abe","101180770b254d31b737857853f906d9"]},"id":"RST5PFgMUN71","executionInfo":{"status":"ok","timestamp":1652981166143,"user_tz":-180,"elapsed":989,"user":{"displayName":"Izabella Bartalus","userId":"14250698818363907854"}},"outputId":"2edd56db-eb02-4eb4-e864-6d97b1e87634"},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e84da1577d0e44e280df7618042ec4e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc7ae6ba276042a082d0e47365c18a40"}},"metadata":{}}]},{"cell_type":"code","source":["history = model.fit(\n","    train_data,\n","    validation_data=valid_data,\n","    epochs=epochs,\n","    use_multiprocessing=True,\n","    workers=-1,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y1cy6L0wUPkg","executionInfo":{"status":"ok","timestamp":1652981225449,"user_tz":-180,"elapsed":57700,"user":{"displayName":"Izabella Bartalus","userId":"14250698818363907854"}},"outputId":"f72f938a-e408-40d7-99b1-7c789c10abdc"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","21/21 [==============================] - ETA: 0s - loss: 0.8408 - acc: 0.7292"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r21/21 [==============================] - 43s 938ms/step - loss: 0.8408 - acc: 0.7292 - val_loss: 0.6988 - val_acc: 0.7748\n","Epoch 2/2\n","21/21 [==============================] - 14s 703ms/step - loss: 0.7342 - acc: 0.7634 - val_loss: 0.7059 - val_acc: 0.7748\n"]}]},{"cell_type":"code","source":["# Unfreeze the bert_model.\n","bert_model.trainable = True\n","# Recompile the model to make the change effective.\n","model.compile(\n","    optimizer=tf.keras.optimizers.Adam(1e-5),\n","    loss=\"categorical_crossentropy\",\n","    metrics=[\"accuracy\"],\n",")\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_sll2VI_U_bn","executionInfo":{"status":"ok","timestamp":1652981265720,"user_tz":-180,"elapsed":443,"user":{"displayName":"Izabella Bartalus","userId":"14250698818363907854"}},"outputId":"5352153e-7add-40f4-8aee-ffcb6762cbcf"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_ids (InputLayer)         [(None, 128)]        0           []                               \n","                                                                                                  \n"," attention_masks (InputLayer)   [(None, 128)]        0           []                               \n","                                                                                                  \n"," token_type_ids (InputLayer)    [(None, 128)]        0           []                               \n","                                                                                                  \n"," tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_ids[0][0]',              \n","                                thPoolingAndCrossAt               'attention_masks[0][0]',        \n","                                tentions(last_hidde               'token_type_ids[0][0]']         \n","                                n_state=(None, 128,                                               \n","                                 768),                                                            \n","                                 pooler_output=(Non                                               \n","                                e, 768),                                                          \n","                                 past_key_values=No                                               \n","                                ne, hidden_states=N                                               \n","                                one, attentions=Non                                               \n","                                e, cross_attentions                                               \n","                                =None)                                                            \n","                                                                                                  \n"," bidirectional (Bidirectional)  (None, 128, 128)     426496      ['tf_bert_model[0][0]']          \n","                                                                                                  \n"," global_average_pooling1d (Glob  (None, 128)         0           ['bidirectional[0][0]']          \n"," alAveragePooling1D)                                                                              \n","                                                                                                  \n"," global_max_pooling1d (GlobalMa  (None, 128)         0           ['bidirectional[0][0]']          \n"," xPooling1D)                                                                                      \n","                                                                                                  \n"," concatenate (Concatenate)      (None, 256)          0           ['global_average_pooling1d[0][0]'\n","                                                                 , 'global_max_pooling1d[0][0]']  \n","                                                                                                  \n"," dropout_37 (Dropout)           (None, 256)          0           ['concatenate[0][0]']            \n","                                                                                                  \n"," dense (Dense)                  (None, 3)            771         ['dropout_37[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 109,909,507\n","Trainable params: 109,909,507\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","source":["history = model.fit(\n","    train_data,\n","    validation_data=valid_data,\n","    epochs=epochs,\n","    use_multiprocessing=True,\n","    workers=-1,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-oaFmqlKVDek","executionInfo":{"status":"ok","timestamp":1652981355618,"user_tz":-180,"elapsed":85266,"user":{"displayName":"Izabella Bartalus","userId":"14250698818363907854"}},"outputId":"12c0a190-cbb7-4d1c-dd2f-5792c6cfa50d"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n","21/21 [==============================] - 49s 1s/step - loss: 0.7165 - accuracy: 0.7664 - val_loss: 0.6584 - val_accuracy: 0.7748\n","Epoch 2/2\n","21/21 [==============================] - 25s 1s/step - loss: 0.6773 - accuracy: 0.7664 - val_loss: 0.6417 - val_accuracy: 0.7748\n"]}]},{"cell_type":"code","source":["test_data = BertSemanticDataGenerator(\n","    test_df[[\"puzzle_text\", \"question\"]].values.astype(\"str\"),\n","    y_test,\n","    batch_size=batch_size,\n","    shuffle=False,\n",")\n","model.evaluate(test_data, verbose=1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M3_TMBuiVGKp","executionInfo":{"status":"ok","timestamp":1652981388458,"user_tz":-180,"elapsed":9595,"user":{"displayName":"Izabella Bartalus","userId":"14250698818363907854"}},"outputId":"b80bdfaf-13d5-49c8-d884-52d01263d174"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["29/29 [==============================] - 9s 290ms/step - loss: 0.6425 - accuracy: 0.7737\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.6425171494483948, 0.7737069129943848]"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["def check_similarity(sentence1, sentence2):\n","    sentence_pairs = np.array([[str(sentence1), str(sentence2)]])\n","    test_data = BertSemanticDataGenerator(\n","        sentence_pairs, labels=None, batch_size=1, shuffle=False, include_targets=False,\n","    )\n","\n","    proba = model.predict(test_data[0])[0]\n","    idx = np.argmax(proba)\n","    proba = f\"{proba[idx]: .2f}%\"\n","    pred = labels[idx]\n","    return pred, proba"],"metadata":{"id":"TYeHfLeKXlw4","executionInfo":{"status":"ok","timestamp":1652981408302,"user_tz":-180,"elapsed":12,"user":{"displayName":"Izabella Bartalus","userId":"14250698818363907854"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["sentence1 = \"On the island where each inhabitant is either a knave or a knight , knights always tell the truth while knaves always lie . You meet three inhabitants : Alice , Rex and Bob .  Rex claims that it is false that Bob is a knave . Bob says that he is a knight or Alice is a knight . Can you determine who is a knight and who is a knave ?\"\n","sentence2 = \"Is Rex the knight ?\"\n","check_similarity(sentence1, sentence2)"],"metadata":{"id":"3wj-rc8XXqjA"},"execution_count":null,"outputs":[]}]}