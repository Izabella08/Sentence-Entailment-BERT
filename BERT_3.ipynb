{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0.ipynb","provenance":[],"mount_file_id":"1QkPDe0M9bFBRePekVg-ztj3sedtL01ZY","authorship_tag":"ABX9TyMpsLd/hGDw53n9GPSpSgkG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install transformers\n","!pip install sentencepiece"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-LbYhzMfS5pC","executionInfo":{"status":"ok","timestamp":1652985364321,"user_tz":-180,"elapsed":5901,"user":{"displayName":"Izabella Bartalus","userId":"14250698818363907854"}},"outputId":"537b0695-57a5-4d11-b0a3-c76af4b1206d"},"execution_count":70,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.19.2)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n"]}]},{"cell_type":"code","execution_count":71,"metadata":{"id":"brKR1YsGSpHB","executionInfo":{"status":"ok","timestamp":1652985364321,"user_tz":-180,"elapsed":24,"user":{"displayName":"Izabella Bartalus","userId":"14250698818363907854"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import transformers"]},{"cell_type":"code","source":["max_length = 128  # Maximum length of input sentence to the model.\n","batch_size = 32\n","epochs = 10\n","\n","# Labels in our dataset.\n","labels = [\"NOT ENTAILMENT - Unknown\", \"Entailment\", \"NOT ENTAILMENT - Contradiction\"]"],"metadata":{"id":"mPMp-xAHTAAR","executionInfo":{"status":"ok","timestamp":1652985364321,"user_tz":-180,"elapsed":23,"user":{"displayName":"Izabella Bartalus","userId":"14250698818363907854"}}},"execution_count":72,"outputs":[]},{"cell_type":"code","source":["PATH = '/content/drive/MyDrive/Colab Notebooks/ambiguous_knights_knaves.json'"],"metadata":{"id":"J9lX78wvTIUh","executionInfo":{"status":"ok","timestamp":1652985364322,"user_tz":-180,"elapsed":23,"user":{"displayName":"Izabella Bartalus","userId":"14250698818363907854"}}},"execution_count":73,"outputs":[]},{"cell_type":"code","source":["import json\n","def read_jsonl_file(PATH):\n","  df = pd.read_json(PATH)\n","\n","  with open(PATH,'r') as f:\n","    data = json.loads(f.read())\n","\n","  df_nested_list = pd.json_normalize(data=data['puzzles'], record_path='QA', meta=['puzzle_text'])\n","  return df_nested_list"],"metadata":{"id":"ukkIuuOxTObP","executionInfo":{"status":"ok","timestamp":1652985364323,"user_tz":-180,"elapsed":22,"user":{"displayName":"Izabella Bartalus","userId":"14250698818363907854"}}},"execution_count":74,"outputs":[]},{"cell_type":"code","source":["df = pd.DataFrame(read_jsonl_file(PATH))"],"metadata":{"id":"BMsfLMcWTd4B","executionInfo":{"status":"ok","timestamp":1652985364324,"user_tz":-180,"elapsed":23,"user":{"displayName":"Izabella Bartalus","userId":"14250698818363907854"}}},"execution_count":75,"outputs":[]},{"cell_type":"code","source":["# SE IMPARTE DATASET-UL IN TRAIN, VALIDARE SI TEST\n","train_df = df[['qid','puzzle_text', 'question', 'answer']][:700]\n","valid_df  = df[['qid','puzzle_text', 'question', 'answer']]\n","test_df  = df[['qid','puzzle_text', 'question', 'answer']]\n","\n","# Shape of the data\n","print(f\"Total train samples : {train_df.shape[0]}\")\n","print(f\"Total validation samples: {valid_df.shape[0]}\")\n","print(f\"Total test samples: {test_df.shape[0]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OhJqTUn7TVB9","executionInfo":{"status":"ok","timestamp":1652985364324,"user_tz":-180,"elapsed":22,"user":{"displayName":"Izabella Bartalus","userId":"14250698818363907854"}},"outputId":"70f198ea-37bd-486e-a2ad-68608367e1f9"},"execution_count":76,"outputs":[{"output_type":"stream","name":"stdout","text":["Total train samples : 700\n","Total validation samples: 940\n","Total test samples: 940\n"]}]},{"cell_type":"code","source":["print(\"Train Target Distribution\")\n","print(train_df.answer.value_counts())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v92cgmPXTjMB","executionInfo":{"status":"ok","timestamp":1652985364325,"user_tz":-180,"elapsed":21,"user":{"displayName":"Izabella Bartalus","userId":"14250698818363907854"}},"outputId":"744d9670-588a-4f98-c2cb-75aba8ac765a"},"execution_count":77,"outputs":[{"output_type":"stream","name":"stdout","text":["Train Target Distribution\n","NOT ENTAILMENT - Unknown          536\n","Entailment                         82\n","NOT ENTAILMENT - Contradiction     82\n","Name: answer, dtype: int64\n"]}]},{"cell_type":"code","source":["print(\"Validation Target Distribution\")\n","print(valid_df.answer.value_counts())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AOoRdf1qTpAs","executionInfo":{"status":"ok","timestamp":1652985364325,"user_tz":-180,"elapsed":18,"user":{"displayName":"Izabella Bartalus","userId":"14250698818363907854"}},"outputId":"ea86475d-9be8-4a83-ad0b-311b1bc8c738"},"execution_count":78,"outputs":[{"output_type":"stream","name":"stdout","text":["Validation Target Distribution\n","NOT ENTAILMENT - Unknown          730\n","Entailment                        105\n","NOT ENTAILMENT - Contradiction    105\n","Name: answer, dtype: int64\n"]}]},{"cell_type":"code","source":["train_df = (\n","    train_df[train_df.answer != \"-\"]\n","    .sample(frac=1.0, random_state=42)\n","    .reset_index(drop=True)\n",")\n","valid_df = (\n","    valid_df[valid_df.answer != \"-\"]\n","    .sample(frac=1.0, random_state=42)\n","    .reset_index(drop=True)\n",")"],"metadata":{"id":"ZbwfjGH2Ttvb","executionInfo":{"status":"ok","timestamp":1652985364326,"user_tz":-180,"elapsed":17,"user":{"displayName":"Izabella Bartalus","userId":"14250698818363907854"}}},"execution_count":79,"outputs":[]},{"cell_type":"code","source":["train_df[\"label\"] = train_df[\"answer\"].apply(\n","    lambda x: 0 if x == \"NOT ENTAILMENT - Contradiction\" else 1 if x == \"Entailment\" else 2\n",")\n","y_train = tf.keras.utils.to_categorical(train_df.label, num_classes=3)\n","\n","valid_df[\"label\"] = valid_df[\"answer\"].apply(\n","    lambda x: 0 if x == \"NOT ENTAILMENT - Contradiction\" else 1 if x == \"Entailment\" else 2\n",")\n","y_val = tf.keras.utils.to_categorical(valid_df.label, num_classes=3)\n","\n","test_df[\"label\"] = test_df[\"answer\"].apply(\n","    lambda x: 0 if x == \"NOT ENTAILMENT - Contradiction\" else 1 if x == \"Entailment\" else 2\n",")\n","y_test = tf.keras.utils.to_categorical(test_df.label, num_classes=3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KBxEdcI0TxL4","executionInfo":{"status":"ok","timestamp":1652985364326,"user_tz":-180,"elapsed":16,"user":{"displayName":"Izabella Bartalus","userId":"14250698818363907854"}},"outputId":"54d0b3e4-cbfb-445e-bb65-20a2b2947a67"},"execution_count":80,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  if sys.path[0] == '':\n"]}]},{"cell_type":"code","source":["class BertSemanticDataGenerator(tf.keras.utils.Sequence):\n","    \"\"\"Generates batches of data.\n","\n","    Args:\n","        sentence_pairs: Array of premise and hypothesis input sentences.\n","        labels: Array of labels.\n","        batch_size: Integer batch size.\n","        shuffle: boolean, whether to shuffle the data.\n","        include_targets: boolean, whether to incude the labels.\n","\n","    Returns:\n","        Tuples `([input_ids, attention_mask, `token_type_ids], labels)`\n","        (or just `[input_ids, attention_mask, `token_type_ids]`\n","         if `include_targets=False`)\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        sentence_pairs,\n","        labels,\n","        batch_size=batch_size,\n","        shuffle=True,\n","        include_targets=True,\n","    ):\n","        self.sentence_pairs = sentence_pairs\n","        self.labels = labels\n","        self.shuffle = shuffle\n","        self.batch_size = batch_size\n","        self.include_targets = include_targets\n","        # Load our BERT Tokenizer to encode the text.\n","        # We will use base-base-uncased pretrained model.\n","        self.tokenizer = transformers.BertTokenizer.from_pretrained(\n","            \"bert-base-uncased\", do_lower_case=True\n","        )\n","        self.indexes = np.arange(len(self.sentence_pairs))\n","        self.on_epoch_end()\n","\n","    def __len__(self):\n","        # Denotes the number of batches per epoch.\n","        return len(self.sentence_pairs) // self.batch_size\n","\n","    def __getitem__(self, idx):\n","        # Retrieves the batch of index.\n","        indexes = self.indexes[idx * self.batch_size : (idx + 1) * self.batch_size]\n","        sentence_pairs = self.sentence_pairs[indexes]\n","\n","        # With BERT tokenizer's batch_encode_plus batch of both the sentences are\n","        # encoded together and separated by [SEP] token.\n","        encoded = self.tokenizer.batch_encode_plus(\n","            sentence_pairs.tolist(),\n","            add_special_tokens=True,\n","            max_length=max_length,\n","            return_attention_mask=True,\n","            return_token_type_ids=True,\n","            pad_to_max_length=True,\n","            return_tensors=\"tf\",\n","        )\n","\n","        # Convert batch of encoded features to numpy array.\n","        input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n","        attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n","        token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n","\n","        # Set to true if data generator is used for training/validation.\n","        if self.include_targets:\n","            labels = np.array(self.labels[indexes], dtype=\"int32\")\n","            return [input_ids, attention_masks, token_type_ids], labels\n","        else:\n","            return [input_ids, attention_masks, token_type_ids]\n","\n","    def on_epoch_end(self):\n","        # Shuffle indexes after each epoch if shuffle is set to True.\n","        if self.shuffle:\n","            np.random.RandomState(42).shuffle(self.indexes)"],"metadata":{"id":"V0ACX8zOT9Re","executionInfo":{"status":"ok","timestamp":1652985364814,"user_tz":-180,"elapsed":501,"user":{"displayName":"Izabella Bartalus","userId":"14250698818363907854"}}},"execution_count":81,"outputs":[]},{"cell_type":"code","source":["# Create the model under a distribution strategy scope.\n","strategy = tf.distribute.MirroredStrategy()\n","\n","with strategy.scope():\n","    # Encoded token ids from BERT tokenizer.\n","    input_ids = tf.keras.layers.Input(\n","        shape=(max_length,), dtype=tf.int32, name=\"input_ids\"\n","    )\n","    # Attention masks indicates to the model which tokens should be attended to.\n","    attention_masks = tf.keras.layers.Input(\n","        shape=(max_length,), dtype=tf.int32, name=\"attention_masks\"\n","    )\n","    # Token type ids are binary masks identifying different sequences in the model.\n","    token_type_ids = tf.keras.layers.Input(\n","        shape=(max_length,), dtype=tf.int32, name=\"token_type_ids\"\n","    )\n","    # Loading pretrained BERT model.\n","    bert_model = transformers.TFBertModel.from_pretrained(\"bert-base-uncased\")\n","    # Freeze the BERT model to reuse the pretrained features without modifying them.\n","    bert_model.trainable = False \n","\n","    bert_output = bert_model(\n","        input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids\n","    )\n","    sequence_output = bert_output.last_hidden_state\n","    pooled_output = bert_output.pooler_output\n","    # Add trainable layers on top of frozen layers to adapt the pretrained features on the new data.\n","    bi_lstm = tf.keras.layers.Bidirectional(\n","        tf.keras.layers.LSTM(64, return_sequences=True)\n","    )(sequence_output)\n","    # Applying hybrid pooling approach to bi_lstm sequence output.\n","    max_pooling = tf.keras.layers.MaxPooling1D(2)(bi_lstm)\n","    dense1 = tf.keras.layers.Dense(1, activation='relu')(max_pooling)  \n","    conv1 = tf.keras.layers.Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(dense1)   \n","    dense2 = tf.keras.layers.Dense(10, activation='sigmoid')(conv1) \n","    conv2 = tf.keras.layers.Conv1D(filters=64, kernel_size=5, padding='same', activation='relu')(dense2)\n","    conv3 = tf.keras.layers.Conv1D(filters=32, kernel_size=5, padding='same', activation='relu')(conv2)\n","    conv4 = tf.keras.layers.Conv1D(filters=16, kernel_size=4, padding='same', activation='relu')(conv3)\n","    dense3 = tf.keras.layers.Dense(20, activation='sigmoid')(conv4) \n","    conv5 = tf.keras.layers.Conv1D(filters=8, kernel_size=3, padding='same', activation='relu')(conv4)\n","    conv6 = tf.keras.layers.Conv1D(filters=4, kernel_size=3, padding='same', activation='relu')(conv5)\n","    dense4 = tf.keras.layers.Dense(20, activation='sigmoid')(conv6) \n","    flatten = tf.keras.layers.Flatten()(dense4)\n","    output = tf.keras.layers.Dense(3, activation=\"softmax\")(flatten)\n","    model2 = tf.keras.models.Model(\n","        inputs=[input_ids, attention_masks, token_type_ids], outputs=output\n","    )\n","\n","    model2.compile(\n","        optimizer=tf.keras.optimizers.Adam(),\n","        loss=\"binary_crossentropy\",       # am schimbat loss function\n","        metrics=[\"acc\"],\n","    )\n","\n","\n","print(f\"Strategy: {strategy}\")\n","model2.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"26pIhg5kUA6R","executionInfo":{"status":"ok","timestamp":1652985369526,"user_tz":-180,"elapsed":4723,"user":{"displayName":"Izabella Bartalus","userId":"14250698818363907854"}},"outputId":"964f8a2a-b9a2-4c8b-80d9-c8a51ac096c3"},"execution_count":82,"outputs":[{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"]},{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n","- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["Strategy: <tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7f80bae24490>\n","Model: \"model_9\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_ids (InputLayer)         [(None, 128)]        0           []                               \n","                                                                                                  \n"," attention_masks (InputLayer)   [(None, 128)]        0           []                               \n","                                                                                                  \n"," token_type_ids (InputLayer)    [(None, 128)]        0           []                               \n","                                                                                                  \n"," tf_bert_model_10 (TFBertModel)  TFBaseModelOutputWi  109482240  ['input_ids[0][0]',              \n","                                thPoolingAndCrossAt               'attention_masks[0][0]',        \n","                                tentions(last_hidde               'token_type_ids[0][0]']         \n","                                n_state=(None, 128,                                               \n","                                 768),                                                            \n","                                 pooler_output=(Non                                               \n","                                e, 768),                                                          \n","                                 past_key_values=No                                               \n","                                ne, hidden_states=N                                               \n","                                one, attentions=Non                                               \n","                                e, cross_attentions                                               \n","                                =None)                                                            \n","                                                                                                  \n"," bidirectional_10 (Bidirectiona  (None, 128, 128)    426496      ['tf_bert_model_10[0][0]']       \n"," l)                                                                                               \n","                                                                                                  \n"," max_pooling1d_7 (MaxPooling1D)  (None, 64, 128)     0           ['bidirectional_10[0][0]']       \n","                                                                                                  \n"," dense_38 (Dense)               (None, 64, 1)        129         ['max_pooling1d_7[0][0]']        \n","                                                                                                  \n"," conv1d_21 (Conv1D)             (None, 64, 32)       128         ['dense_38[0][0]']               \n","                                                                                                  \n"," dense_39 (Dense)               (None, 64, 10)       330         ['conv1d_21[0][0]']              \n","                                                                                                  \n"," conv1d_22 (Conv1D)             (None, 64, 64)       3264        ['dense_39[0][0]']               \n","                                                                                                  \n"," conv1d_23 (Conv1D)             (None, 64, 32)       10272       ['conv1d_22[0][0]']              \n","                                                                                                  \n"," conv1d_24 (Conv1D)             (None, 64, 16)       2064        ['conv1d_23[0][0]']              \n","                                                                                                  \n"," conv1d_25 (Conv1D)             (None, 64, 8)        392         ['conv1d_24[0][0]']              \n","                                                                                                  \n"," conv1d_26 (Conv1D)             (None, 64, 4)        100         ['conv1d_25[0][0]']              \n","                                                                                                  \n"," dense_41 (Dense)               (None, 64, 20)       100         ['conv1d_26[0][0]']              \n","                                                                                                  \n"," flatten_9 (Flatten)            (None, 1280)         0           ['dense_41[0][0]']               \n","                                                                                                  \n"," dense_42 (Dense)               (None, 3)            3843        ['flatten_9[0][0]']              \n","                                                                                                  \n","==================================================================================================\n","Total params: 109,929,358\n","Trainable params: 447,118\n","Non-trainable params: 109,482,240\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","source":["train_data = BertSemanticDataGenerator(\n","    train_df[[\"puzzle_text\", \"question\"]].values.astype(\"str\"),\n","    y_train,\n","    batch_size=batch_size,\n","    shuffle=True,\n",")\n","valid_data = BertSemanticDataGenerator(\n","    valid_df[[\"puzzle_text\", \"question\"]].values.astype(\"str\"),\n","    y_val,\n","    batch_size=batch_size,\n","    shuffle=False,\n",")"],"metadata":{"id":"RST5PFgMUN71","executionInfo":{"status":"ok","timestamp":1652985373201,"user_tz":-180,"elapsed":3679,"user":{"displayName":"Izabella Bartalus","userId":"14250698818363907854"}}},"execution_count":83,"outputs":[]},{"cell_type":"code","source":["history = model2.fit(\n","    train_data,\n","    validation_data=valid_data,\n","    epochs=epochs,\n","    use_multiprocessing=True,\n","    workers=-1,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y1cy6L0wUPkg","executionInfo":{"status":"ok","timestamp":1652985561682,"user_tz":-180,"elapsed":188497,"user":{"displayName":"Izabella Bartalus","userId":"14250698818363907854"}},"outputId":"01e102e7-fe14-489f-80ed-6563c5cc8fe7"},"execution_count":84,"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","21/21 [==============================] - ETA: 0s - loss: 0.4558 - acc: 0.7634"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r21/21 [==============================] - 43s 974ms/step - loss: 0.4558 - acc: 0.7634 - val_loss: 0.4193 - val_acc: 0.7748\n","Epoch 2/10\n","21/21 [==============================] - 16s 759ms/step - loss: 0.4324 - acc: 0.7634 - val_loss: 0.4170 - val_acc: 0.7748\n","Epoch 3/10\n","21/21 [==============================] - 17s 847ms/step - loss: 0.4333 - acc: 0.7664 - val_loss: 0.4254 - val_acc: 0.7748\n","Epoch 4/10\n","21/21 [==============================] - 16s 751ms/step - loss: 0.4299 - acc: 0.7664 - val_loss: 0.4239 - val_acc: 0.7748\n","Epoch 5/10\n","21/21 [==============================] - 15s 750ms/step - loss: 0.4305 - acc: 0.7634 - val_loss: 0.4193 - val_acc: 0.7748\n","Epoch 6/10\n","21/21 [==============================] - 15s 754ms/step - loss: 0.4383 - acc: 0.7634 - val_loss: 0.4293 - val_acc: 0.7748\n","Epoch 7/10\n","21/21 [==============================] - 16s 756ms/step - loss: 0.4300 - acc: 0.7664 - val_loss: 0.4135 - val_acc: 0.7748\n","Epoch 8/10\n","21/21 [==============================] - 16s 756ms/step - loss: 0.4261 - acc: 0.7679 - val_loss: 0.4197 - val_acc: 0.7748\n","Epoch 9/10\n","21/21 [==============================] - 16s 755ms/step - loss: 0.4311 - acc: 0.7619 - val_loss: 0.4165 - val_acc: 0.7748\n","Epoch 10/10\n","21/21 [==============================] - 15s 753ms/step - loss: 0.4275 - acc: 0.7679 - val_loss: 0.4283 - val_acc: 0.7748\n"]}]},{"cell_type":"code","source":["# Unfreeze the bert_model.\n","bert_model.trainable = True\n","# Recompile the model to make the change effective.\n","model2.compile(\n","    optimizer=tf.keras.optimizers.Adam(1e-5),\n","    loss=\"binary_crossentropy\",\n","    metrics=[\"accuracy\"],\n",")\n","model2.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_sll2VI_U_bn","executionInfo":{"status":"ok","timestamp":1652985561682,"user_tz":-180,"elapsed":50,"user":{"displayName":"Izabella Bartalus","userId":"14250698818363907854"}},"outputId":"06d63dd2-41ea-48b4-b9c4-d281b2bf6acf"},"execution_count":85,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_9\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_ids (InputLayer)         [(None, 128)]        0           []                               \n","                                                                                                  \n"," attention_masks (InputLayer)   [(None, 128)]        0           []                               \n","                                                                                                  \n"," token_type_ids (InputLayer)    [(None, 128)]        0           []                               \n","                                                                                                  \n"," tf_bert_model_10 (TFBertModel)  TFBaseModelOutputWi  109482240  ['input_ids[0][0]',              \n","                                thPoolingAndCrossAt               'attention_masks[0][0]',        \n","                                tentions(last_hidde               'token_type_ids[0][0]']         \n","                                n_state=(None, 128,                                               \n","                                 768),                                                            \n","                                 pooler_output=(Non                                               \n","                                e, 768),                                                          \n","                                 past_key_values=No                                               \n","                                ne, hidden_states=N                                               \n","                                one, attentions=Non                                               \n","                                e, cross_attentions                                               \n","                                =None)                                                            \n","                                                                                                  \n"," bidirectional_10 (Bidirectiona  (None, 128, 128)    426496      ['tf_bert_model_10[0][0]']       \n"," l)                                                                                               \n","                                                                                                  \n"," max_pooling1d_7 (MaxPooling1D)  (None, 64, 128)     0           ['bidirectional_10[0][0]']       \n","                                                                                                  \n"," dense_38 (Dense)               (None, 64, 1)        129         ['max_pooling1d_7[0][0]']        \n","                                                                                                  \n"," conv1d_21 (Conv1D)             (None, 64, 32)       128         ['dense_38[0][0]']               \n","                                                                                                  \n"," dense_39 (Dense)               (None, 64, 10)       330         ['conv1d_21[0][0]']              \n","                                                                                                  \n"," conv1d_22 (Conv1D)             (None, 64, 64)       3264        ['dense_39[0][0]']               \n","                                                                                                  \n"," conv1d_23 (Conv1D)             (None, 64, 32)       10272       ['conv1d_22[0][0]']              \n","                                                                                                  \n"," conv1d_24 (Conv1D)             (None, 64, 16)       2064        ['conv1d_23[0][0]']              \n","                                                                                                  \n"," conv1d_25 (Conv1D)             (None, 64, 8)        392         ['conv1d_24[0][0]']              \n","                                                                                                  \n"," conv1d_26 (Conv1D)             (None, 64, 4)        100         ['conv1d_25[0][0]']              \n","                                                                                                  \n"," dense_41 (Dense)               (None, 64, 20)       100         ['conv1d_26[0][0]']              \n","                                                                                                  \n"," flatten_9 (Flatten)            (None, 1280)         0           ['dense_41[0][0]']               \n","                                                                                                  \n"," dense_42 (Dense)               (None, 3)            3843        ['flatten_9[0][0]']              \n","                                                                                                  \n","==================================================================================================\n","Total params: 109,929,358\n","Trainable params: 109,929,358\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","source":["history = model2.fit(\n","    train_data,\n","    validation_data=valid_data,\n","    epochs=epochs,\n","    use_multiprocessing=True,\n","    workers=-1,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-oaFmqlKVDek","executionInfo":{"status":"ok","timestamp":1652985838279,"user_tz":-180,"elapsed":276607,"user":{"displayName":"Izabella Bartalus","userId":"14250698818363907854"}},"outputId":"34c0f6c1-ce71-4b81-c095-fc0ae9d68747"},"execution_count":86,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_10/bert/pooler/dense/kernel:0', 'tf_bert_model_10/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_10/bert/pooler/dense/kernel:0', 'tf_bert_model_10/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n","21/21 [==============================] - 51s 2s/step - loss: 0.4297 - accuracy: 0.7693 - val_loss: 0.4223 - val_accuracy: 0.7748\n","Epoch 2/10\n","21/21 [==============================] - 25s 1s/step - loss: 0.4280 - accuracy: 0.7664 - val_loss: 0.4192 - val_accuracy: 0.7748\n","Epoch 3/10\n","21/21 [==============================] - 25s 1s/step - loss: 0.4312 - accuracy: 0.7604 - val_loss: 0.4171 - val_accuracy: 0.7748\n","Epoch 4/10\n","21/21 [==============================] - 25s 1s/step - loss: 0.4259 - accuracy: 0.7649 - val_loss: 0.4154 - val_accuracy: 0.7748\n","Epoch 5/10\n","21/21 [==============================] - 25s 1s/step - loss: 0.4204 - accuracy: 0.7693 - val_loss: 0.4146 - val_accuracy: 0.7748\n","Epoch 6/10\n","21/21 [==============================] - 25s 1s/step - loss: 0.4227 - accuracy: 0.7664 - val_loss: 0.4139 - val_accuracy: 0.7748\n","Epoch 7/10\n","21/21 [==============================] - 25s 1s/step - loss: 0.4222 - accuracy: 0.7664 - val_loss: 0.4135 - val_accuracy: 0.7748\n","Epoch 8/10\n","21/21 [==============================] - 25s 1s/step - loss: 0.4235 - accuracy: 0.7649 - val_loss: 0.4134 - val_accuracy: 0.7748\n","Epoch 9/10\n","21/21 [==============================] - 25s 1s/step - loss: 0.4173 - accuracy: 0.7708 - val_loss: 0.4129 - val_accuracy: 0.7748\n","Epoch 10/10\n","21/21 [==============================] - 25s 1s/step - loss: 0.4187 - accuracy: 0.7693 - val_loss: 0.4127 - val_accuracy: 0.7748\n"]}]},{"cell_type":"code","source":["test_data = BertSemanticDataGenerator(\n","    test_df[[\"puzzle_text\", \"question\"]].values.astype(\"str\"),\n","    y_test,\n","    batch_size=batch_size,\n","    shuffle=False,\n",")\n","model2.evaluate(test_data, verbose=1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M3_TMBuiVGKp","executionInfo":{"status":"ok","timestamp":1652985850634,"user_tz":-180,"elapsed":12383,"user":{"displayName":"Izabella Bartalus","userId":"14250698818363907854"}},"outputId":"e264feee-439c-4d65-dd6e-de8bf827c3ec"},"execution_count":87,"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["29/29 [==============================] - 9s 294ms/step - loss: 0.4139 - accuracy: 0.7737\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.41388189792633057, 0.7737069129943848]"]},"metadata":{},"execution_count":87}]},{"cell_type":"code","source":["def check_similarity(sentence1, sentence2):\n","    sentence_pairs = np.array([[str(sentence1), str(sentence2)]])\n","    test_data = BertSemanticDataGenerator(\n","        sentence_pairs, labels=None, batch_size=1, shuffle=False, include_targets=False,\n","    )\n","\n","    proba = model2.predict(test_data[0])[0]\n","    idx = np.argmax(proba)\n","    proba = f\"{proba[idx]: .2f}%\"\n","    pred = labels[idx]\n","    return pred, proba"],"metadata":{"id":"TYeHfLeKXlw4","executionInfo":{"status":"ok","timestamp":1652985850634,"user_tz":-180,"elapsed":5,"user":{"displayName":"Izabella Bartalus","userId":"14250698818363907854"}}},"execution_count":88,"outputs":[]},{"cell_type":"code","source":["sentence1 = \"On the island where each inhabitant is either a knave or a knight , knights always tell the truth while knaves always lie . You meet three inhabitants : Alice , Rex and Bob .  Rex claims that it is false that Bob is a knave . Bob says that he is a knight or Alice is a knight . Can you determine who is a knight and who is a knave ?\"\n","sentence2 = \"Is Rex the knight ?\"\n","check_similarity(sentence1, sentence2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3wj-rc8XXqjA","executionInfo":{"status":"ok","timestamp":1652985857181,"user_tz":-180,"elapsed":6551,"user":{"displayName":"Izabella Bartalus","userId":"14250698818363907854"}},"outputId":"2ba70b58-f532-46c1-fcc3-a4c07f3a6bfe"},"execution_count":89,"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:5 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f80ba8a5560> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"execute_result","data":{"text/plain":["('NOT ENTAILMENT - Contradiction', ' 0.92%')"]},"metadata":{},"execution_count":89}]}]}